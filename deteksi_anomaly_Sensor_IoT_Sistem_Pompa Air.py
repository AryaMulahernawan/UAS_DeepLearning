# -*- coding: utf-8 -*-
"""Another copy of deteksi anomaly sensor suhu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1anRhs4BTooHo3E20Ene7q0j986AEQCbj

# **1. Import Library**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization,LSTM
from tensorflow.keras.callbacks import EarlyStopping
from google.colab import drive
import json
import kagglehub
import tensorflow as tf
from tensorflow.keras.regularizers import l2
from sklearn.utils.class_weight import compute_class_weight
from sklearn.svm import SVC
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.metrics import precision_recall_curve, average_precision_score

from tensorflow.keras.optimizers import Adam

"""# **2. Memuat Dataset**"""

path = kagglehub.dataset_download("nphantawee/pump-sensor-data")

print("Path to dataset files:", path)

csv_path = os.path.join(path, "sensor.csv")
data_sensor = pd.read_csv(csv_path)

data_sensor.head()

"""# **3. Data Preprocessing**"""

print(data_sensor.columns)
print(data_sensor['machine_status'].value_counts())

data_sensor.info()

data_sensor['timestamp'] = pd.to_datetime(data_sensor['timestamp'])

label_map = {
    'NORMAL': 0,
    'RECOVERING': 1,
    'BROKEN': 1
}

data_sensor['label'] = data_sensor['machine_status'].map(label_map)

data_sensor['label'].value_counts()



data_sensor.label.value_counts()

data_sensor.describe()

palette_colors = {"0": "green", "1": "red",}
sns.countplot(x='label', data=data_sensor, palette=palette_colors)
plt.title('Class Distribution of label')
plt.xlabel('label')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(15,5))


plt.plot(
    data_sensor['timestamp'],
    data_sensor['sensor_00'],
    label='machine_status',
    linewidth=1
)


anomaly_points = data_sensor[data_sensor['label'] == 1]
plt.scatter(
    anomaly_points['timestamp'],
    anomaly_points['sensor_00'],
    color='red',
    label='Anomaly',
    s=11
)

plt.title("machine_status")
plt.xlabel("Time")
plt.ylabel("machine_status")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(15,5))


plt.plot(
    data_sensor['timestamp'],
    data_sensor['sensor_01'],
    label='machine_status',
    linewidth=1
)


anomaly_points = data_sensor[data_sensor['label'] == 1]
plt.scatter(
    anomaly_points['timestamp'],
    anomaly_points['sensor_01'],
    color='red',
    label='Anomaly',
    s=11
)

plt.title("machine_status")
plt.xlabel("Time")
plt.ylabel("machine_status")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(15,5))

plt.plot(
    data_sensor['timestamp'],
    data_sensor['sensor_51'],
    label='machine_status',
    linewidth=1
)


anomaly_points = data_sensor[data_sensor['label'] == 1]
plt.scatter(
    anomaly_points['timestamp'],
    anomaly_points['sensor_51'],
    color='red',
    label='Anomaly',
    s=11
)

plt.title("machine_status")
plt.xlabel("Time")
plt.ylabel("machine_status")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

print(data_sensor.columns)

print("Shape of data:", data_sensor.shape)

print(data_sensor['label'].value_counts(normalize=True))

print("Missing values per column:\n", data_sensor.isnull().sum())

data_sensor = data_sensor.drop(columns=['sensor_15'])

data_sensor = data_sensor.dropna()

print("Missing values per column:\n", data_sensor.isnull().sum())

print("Number of duplicates:", data_sensor.duplicated().sum())

sensor_cols = [c for c in data_sensor.columns if c.startswith("sensor_")]
X_raw = data_sensor[sensor_cols].values
y_raw = data_sensor['label'].values

"""# **4. Sliding Window**"""

WINDOW_SIZE = 30
STRIDE = 1

def sliding_window_multivariate(X, y, window_size, stride=1, anomaly_ratio=0.2):
    X_windows, y_windows = [], []

    for i in range(0, len(X) - window_size + 1, stride):
        window_X = X[i:i+window_size]
        window_y = y[i:i+window_size]

        # window anomali jika â‰¥20% titik anomali
        label = 1 if window_y.mean() >= anomaly_ratio else 0

        X_windows.append(window_X)
        y_windows.append(label)

    return np.array(X_windows), np.array(y_windows)

X, y = sliding_window_multivariate(
    X_raw, y_raw,
    window_size=30,
    stride=1,
    anomaly_ratio=0.2
)


print("X:", X.shape)
print("y:", y.shape)

"""# **5. Data Splitting**"""

n = len(X)
train_end = int(0.7 * n)
val_end = int(0.85 * n)

X_train, y_train = X[:train_end], y[:train_end]
X_val, y_val     = X[train_end:val_end], y[train_end:val_end]
X_test, y_test   = X[val_end:], y[val_end:]

print("Train:", X_train.shape)
print("Val  :", X_val.shape)
print("Test :", X_test.shape)

print("Jumlah anomali di val:", y_val.sum())
print("Total val sample:", len(y_val))

"""# **6. Normalisasi**"""

scaler = MinMaxScaler()


X_train_r = X_train.reshape(-1, X_train.shape[-1])
X_val_r   = X_val.reshape(-1, X_val.shape[-1])
X_test_r  = X_test.reshape(-1, X_test.shape[-1])

scaler.fit(X_train_r)

X_train = scaler.transform(X_train_r).reshape(X_train.shape)
X_val   = scaler.transform(X_val_r).reshape(X_val.shape)
X_test  = scaler.transform(X_test_r).reshape(X_test.shape)

print(f"Min nilai data training : {X_train.min()}")
print(f"Max nilai data training : {X_train.max()}")

print(f"Min nilai data validation : {X_test.min()}")
print(f"Max nilai data validation : {X_test.max()}")

print(f"Min nilai data testing : {X_val.min()}")
print(f"Max nilai data testing : {X_val.max()}")

print("X_train:", X_train.shape)
print("X_val  :", X_val.shape)
print("X_test :", X_test.shape)

"""# **7. Membangun Model Hybrid CNN-LSTM**

"""

model = Sequential([

    Conv1D(64, 3, activation='relu', padding='same',
           kernel_regularizer=l2(1e-4),
           input_shape=(X_train.shape[1], X_train.shape[2])),
    BatchNormalization(),
    MaxPooling1D(2),
    Dropout(0.3),

    Conv1D(128, 3, activation='relu', padding='same',
           kernel_regularizer=l2(1e-4)),
    BatchNormalization(),
    MaxPooling1D(2),
    Dropout(0.3),

    LSTM(
        64,
        activation="tanh",
        recurrent_activation="sigmoid",
        dropout=0.2,
        recurrent_dropout=0.2,
        return_sequences=False,
        kernel_regularizer=l2(1e-4)
    ),


    Dropout(0.3),

    Dense(64, activation='relu'),
    Dropout(0.2),

    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=[
        'accuracy'
    ]
)

model.summary()

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

checkpoint = tf.keras.callbacks.ModelCheckpoint(
    "cnn_lstm_best.keras",
    monitor='val_loss',
    save_best_only=True
)

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)

class_weight_dict = {
    0: class_weights[0],
    1: class_weights[1]
}

print("Class weights:", class_weight_dict)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=20,
    batch_size=64,
    callbacks=[early_stop, checkpoint],
    class_weight=class_weight_dict,
    verbose=1
)

"""### Evaluasi Model"""

plt.figure(figsize=(6,4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')

plt.title("Training vs Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(6,4))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')

plt.title("Training vs Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

results = model.evaluate(X_test, y_test, verbose=0)

print(f"Test Loss     : {results[0]:.4f}")
print(f"Test Accuracy : {results[1]:.4f}")

y_pred_prob = model.predict(X_test)


THRESHOLD = 0.5
y_pred = (y_pred_prob >= THRESHOLD).astype(int).ravel()

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=['Normal', 'Anomaly'],
    yticklabels=['Normal', 'Anomaly']
)

plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - CNN LSTM Anomaly Detection")
plt.tight_layout()
plt.show()

y_prob = model.predict(X_test).ravel()

THRESHOLD = 0.5

y_pred = (y_prob >= THRESHOLD).astype(int)

print(
    classification_report(
        y_test,
        y_pred,
        target_names=["Normal", "Anomaly"],
        digits=4
    )
)

y_test_proba = model.predict(X_test).ravel()

roc_auc = roc_auc_score(y_test, y_test_proba)
print(f"ROC-AUC Score: {roc_auc:.4f}")

fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)

plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], linestyle='--')  # random line

plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate (Recall)")
plt.title("ROC Curve - CNN LSTM Anomaly Detection")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""
# **8. Hyperparameter tuning dengan RANDOM SEARCH**



"""

!pip install -U keras-tuner

import keras_tuner as kt

kt.RandomSearch

def build_model(hp):
    model = Sequential()

    # ===== CNN Block 1 =====
    model.add(
        Conv1D(
            filters=hp.Choice('filters_1', [64], default=64),
            kernel_size=hp.Choice('kernel_1', [3], default=3),
            activation='relu',
            padding='same',
            kernel_regularizer=l2(
                hp.Choice('l2_1', [1e-4], default=1e-4)
            ),
            input_shape=(X_train.shape[1], X_train.shape[2])
        )
    )
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=2))
    model.add(Dropout(
        hp.Float('dropout_1', 0.2, 0.5, step=0.1, default=0.3)
    ))

    # ===== CNN Block 2 =====
    model.add(
        Conv1D(
            filters=hp.Choice('filters_2', [128], default=128),
            kernel_size=hp.Choice('kernel_2', [3], default=3),
            activation='relu',
            padding='same',
            kernel_regularizer=l2(
                hp.Choice('l2_2', [1e-4], default=1e-4)
            )
        )
    )
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=2))
    model.add(Dropout(
        hp.Float('dropout_2', 0.2, 0.5, step=0.1, default=0.3)
    ))

    # ===== LSTM =====
    model.add(
        LSTM(
            units=hp.Choice('lstm_units', [64], default=64),
            activation="tanh",
            recurrent_activation="sigmoid",
            dropout=hp.Float('dropout_lstm', 0.2, 0.5, step=0.1, default=0.2),
            recurrent_dropout=0.2,
            return_sequences=False,
            kernel_regularizer=l2(
                hp.Choice('l2_lstm', [1e-4], default=1e-4)
            )
        )
    )
    model.add(Dropout(0.3))

    # ===== Dense =====
    model.add(
        Dense(
            hp.Choice('dense_units', [64], default=64),
            activation='relu'
        )
    )
    model.add(Dropout(0.2))

    model.add(Dense(1, activation='sigmoid'))


    lr = hp.Choice("learning_rate", [1e-4, 3e-4, 1e-3], default=1e-3)

    model.compile(
        optimizer=Adam(learning_rate=lr),
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )

    return model

tuner = kt.RandomSearch(
    build_model,
    objective=kt.Objective("val_loss", direction="min"),
    max_trials=5,
    executions_per_trial=1,
    directory="tuning",
    project_name="cnn_lstm_anomaly"
)

tuner.search(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=20,
    batch_size=64,
    callbacks=[
        EarlyStopping(
            monitor="val_loss",
            patience=5,
            restore_best_weights=True
        )
    ]
)

"""### Evaluasi model Hyperparameter tuning"""

best_model = tuner.get_best_models(num_models=1)[0]
best_model.summary()

test_results = best_model.evaluate(X_test, y_test, verbose=0)

for name, val in zip(best_model.metrics_names, test_results):
    print(f"{name}: {val:.4f}")

y_prob = best_model.predict(X_test).ravel()

threshold = 0.5
y_pred = (y_prob >= threshold).astype(int)

threshold = np.percentile(y_prob, 95)
y_pred = (y_prob >= threshold).astype(int)

print(classification_report(
    y_test, y_pred,
    target_names=["Normal", "Anomaly"]
))

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(5,4))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=["Normal", "Anomaly"],
    yticklabels=["Normal", "Anomaly"]
)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(5,4))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.3f}")
plt.plot([0,1], [0,1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.grid()
plt.show()

precision, recall, _ = precision_recall_curve(y_test, y_prob)
ap = average_precision_score(y_test, y_prob)

plt.figure(figsize=(5,4))
plt.plot(recall, precision, label=f"AP = {ap:.3f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.grid()
plt.show()

"""
# **9. Membangun Model SVM**
"""

X_train_svm = X_train.reshape(X_train.shape[0], -1)
X_val_svm   = X_val.reshape(X_val.shape[0], -1)
X_test_svm  = X_test.reshape(X_test.shape[0], -1)

svm_model = SVC(
    kernel='rbf',
    C=1.0,
    gamma='scale',
    class_weight='balanced'
)

svm_model.fit(X_train_svm, y_train)

"""### Evaluasi Model SVM"""

y_pred_svm = svm_model.predict(X_test_svm)

print("=== SVM Classification Report ===")
print(classification_report(y_test, y_pred_svm, digits=4))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))

y_pred_svm = svm_model.predict(X_test_svm)


cm = confusion_matrix(y_test, y_pred_svm)


plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - SVM')
plt.show()


print("=== SVM Classification Report ===")
print(classification_report(y_test, y_pred_svm, digits=4))

y_score = svm_model.decision_function(X_test_svm)


fpr, tpr, thresholds = roc_curve(y_test, y_score)


roc_auc = auc(fpr, tpr)

print(f"ROC-AUC Score (SVM): {roc_auc:.4f}")


plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f'SVM (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - SVM')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""
# **10. Membangun Model Random Forest**
"""

X_train_rf = X_train.reshape(X_train.shape[0], -1)
X_val_rf   = X_val.reshape(X_val.shape[0], -1)
X_test_rf  = X_test.reshape(X_test.shape[0], -1)

rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train_rf, y_train)

"""### Evaluasi Model Random Forest"""

y_pred_rf = rf_model.predict(X_test_rf)

print("=== Random Forest Classification Report ===")
print(classification_report(y_test, y_pred_rf, digits=4))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))

cm = confusion_matrix(y_test, y_pred_rf)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Random Forest')
plt.show()

y_proba_rf = rf_model.predict_proba(X_test_rf)[:, 1]


roc_auc = roc_auc_score(y_test, y_proba_rf)
print(f"ROC-AUC Score (Random Forest): {roc_auc:.4f}")


fpr, tpr, _ = roc_curve(y_test, y_proba_rf)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f'Random Forest (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Random Forest')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

"""
# **12. Perbandingan Model Hybrid CNN-LSTM, SVM dan Random Forest**
"""

y_proba_dl = model.predict(X_test).ravel()
y_score_svm = svm_model.decision_function(X_test_svm)
y_proba_rf = rf_model.predict_proba(X_test_rf)[:, 1]

plt.figure(figsize=(7,5))

bars = plt.bar(df_results["Model"], df_results["Accuracy"])


for bar in bars:
    yval = bar.get_height()
    plt.text(
        bar.get_x() + bar.get_width()/2,
        yval + 0.01,
        f"{yval:.3f}",
        ha='center',
        va='bottom'
    )

plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.title("Accuracy Comparison of CNN-LSTM, SVM, and Random Forest")
plt.ylim(0, 1)
plt.grid(axis='y')
plt.show()

"""
# **13. Eksport model**
"""

model.save("cnn_lstm_anomaly.h5")

model.export('saved_model/')

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]
tflite_model = converter.convert()

with open("saved_model/model.tflite", "wb") as f:
    f.write(tflite_model)

interpreter = tf.lite.Interpreter(model_path="model.tflite")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print("Input:", input_details)
print("Output:", output_details)

!zip -r saved_model.zip saved_model/

from google.colab import files
files.download('saved_model.zip')

import pickle

with open('scaler_2.pkl', 'wb') as file:
    pickle.dump(scaler, file)







"""Demo Model Masih Proses

# **14. Demo Model**
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import joblib

interpreter = tf.lite.Interpreter(
    model_path="model.tflite"
)
interpreter.allocate_tensors()


input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()


scaler = joblib.load("scaler_2.pkl")

def create_sliding_window(data, window_size):
    X = []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size])
    return np.array(X, dtype=np.float32)

df = pd.read_csv("sensor.csv")

df = df.drop(columns=["timestamp", "machine_status","sensor_15"])
df = df.iloc[:, 1:]
data_new = df.values
data_scaled = scaler.transform(data_new)

WINDOW_SIZE = 30

X_infer = create_sliding_window(data_scaled, WINDOW_SIZE)

print("Shape input:", X_infer.shape)

predictions = []

for i in range(len(X_infer)):
    input_data = np.expand_dims(X_infer[i], axis=0)

    interpreter.set_tensor(
        input_details[0]['index'],
        input_data
    )

    interpreter.invoke()

    output = interpreter.get_tensor(
        output_details[0]['index']
    )

    predictions.append(output[0][0])

predictions = np.array(predictions)

threshold = 0.5

labels = (predictions > threshold).astype(int)

results = pd.DataFrame({
    "Probability": predictions,
    "Label": labels
})

results["Status"] = results["Label"].map({
    0: "Normal",
    1: "Anomali"
})

results.head()

def predict_tflite_single_window(sensor_window):
    """
    sensor_window shape: (window_size, num_features)
    """
    sensor_window = scaler.transform(sensor_window)
    sensor_window = sensor_window.astype(np.float32)
    sensor_window = np.expand_dims(sensor_window, axis=0)

    interpreter.set_tensor(
        input_details[0]['index'],
        sensor_window
    )
    interpreter.invoke()

    prob = interpreter.get_tensor(
        output_details[0]['index']
    )[0][0]

    status = "Anomali" if prob > 0.5 else "Normal"
    return prob, status

prob, status = predict_tflite_single_window(data_new[:30])
print(prob, status)

sensor_window = X_infer[0]

sensor_window_anomali = sensor_window.copy()
sensor_window_anomali[:, 0] *= 10

prob, status = predict_tflite_single_window(sensor_window_anomali)
print(prob, status)